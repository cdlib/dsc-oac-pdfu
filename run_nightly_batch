#!/bin/env python
# -*- coding: utf-8 -*-

import argparse
from lxml.html import parse
import requests
import os, sys
import boto.utils
import boto
import datetime
from time import sleep
from fabric.api import env, run, sudo, put
import paramiko
import fabric
import StringIO
import socket
import urlparse
import tarfile
import time
import tempfile
import shutil
from collections import namedtuple

from pprint import pprint as pp

BATCH = datetime.datetime.now().isoformat()


def main(argv=None):
    """ run a batch of PDF generation

    keeps an amazon simple DB with an inventory of EAD files and last
    modified dates, this persists between runs

    check_url is a recursive modifier function (with side effects) that is
    a web crawler that adds all the new EAD to an array `files_to_generate`
    and updates the amazon simple db as needed

    If there are files_to_generate, a spot purchase of an 8 core 30G RAM 
    is initiated, and fabric is used to install `pdfu` and run the batch
    in parallel using the -P option on xargs.

    Once the batch has run, the spot machine is terminated.

    The shadow file is regenerated even if there were no files to
    generate detected.

    July 1, 2013 is hardcoded as the epic for file changes, because a
    complete batch was run at this time.

    Epic reset to Dec 1, 2013 and simbledb domain re-set in order to
    regenerate some backfiles.  Need to switch simble db to record what
    actually gets created; not what gets batched.

    Epic reset to October 1, 2014.  --generate-all added to rebuild files

    """

    parser = argparse.ArgumentParser(description="run the PDF batch")
    parser.add_argument('eads', nargs=1, help="URL for crawler to start harvesting EAD XML, won't follow redirects")
    parser.add_argument('bucket', nargs=1, help="s3://bucket[/optional/path] where the generated PDF files go")
    parser.add_argument('shadow', nargs=1, help=".tar.gz filename to store \"shadow\" file archive for XTF")
    parser.add_argument('--simpledb-domain', default='ead_last_modified', required=False, help="\"domain\"/name of Amazon Simple DB database")
    parser.add_argument('--shadow-prefix', default='pdf-shadow', required=False, help="path the .tar.gz will unpack to")
    parser.add_argument('--generate-all', dest='all', action='store_true')

    if argv is None:
        argv = parser.parse_args()

    print BATCH
    sdb = boto.connect_sdb()  	# amazon simple db
    last_modified_domain = sdb.get_domain(argv.simpledb_domain)
    files_to_generate = []

    print("checking for files to generate")

    check_url(
        argv.eads[0],
        last_modified_domain,
        files_to_generate,
        generate_all=argv.all,
    )

    if files_to_generate:
        print "there are files to generate"

        batch = generate_batch(
            files_to_generate,
            argv.eads[0],
            argv.bucket[0],
        )
        print "batch generated"

        instance, hostname = launch_ec2()
        print "workhost launched |{0}| |{1}|".format(instance, hostname)

        poll_for_ssh(hostname)
        print "can contact host with ssh to {0}".format(hostname)

        remote_process_pdf(hostname, batch, instance)

        print "okay; done, terminate workhost"
        terminate_ec2(instance)

    print "updating shadow file"
    shadow(argv.bucket[0], argv.shadow[0], argv.shadow_prefix)

    
def check_url(url, last_modified_domain, files_to_generate, generate_all=False):
    """check if a URL is an XML file or directory based on the string value """
    dir, ext = os.path.splitext(url)
    # prime 2002 directory will have only .xml files or sub-directories
    if ext == '.xml':
        check_xml(url, last_modified_domain, files_to_generate, generate_all=generate_all)
    elif not ext:
        check_dir(url, last_modified_domain, files_to_generate)


def check_dir(url, last_modified_domain, files_to_generate):
    """scrape links from directory listing"""
    sys.stdout.write('•')
    doc = parse(url).getroot()
    doc.make_links_absolute()
    links = doc.xpath("//a[@href]/@href")
    for link in links:
        # skip links back to myself and don't go up directories
        if not link == url and link.startswith(url):
            check_url(link, last_modified_domain, files_to_generate)


def check_xml(url, last_modified_domain, files_to_generate, generate_all=False):
    """compare last_modifed in head with value in  simple DB to see if this needs processing"""
    r = requests.head(url)
    last_modified_on_oac_header = r.headers['last-modified']
    last_modified_on_oac = boto.utils.parse_ts(last_modified_on_oac_header)
    # if the URL is not in the last_modified_domain 
    # OR last-modified is later than the database; 
    # add the file to the list of files to be processed
    #
    # check here for failed batches and add
    last_modified_item = last_modified_domain.get_item(url)
    if generate_all:
        add_to_list(url, last_modified_domain, files_to_generate, last_modified_on_oac_header)
    elif not last_modified_item:
        if last_modified_on_oac > datetime.datetime(2014,10,01):
            add_to_list(url, last_modified_domain, files_to_generate, last_modified_on_oac_header)
    elif last_modified_on_oac > boto.utils.parse_ts(last_modified_item["last_modified"]):
        add_to_list(url, last_modified_domain, files_to_generate, last_modified_on_oac_header)
        

def add_to_list(url, last_modified_domain, files_to_generate, last_modified_on_oac_header):
    """modify the files_to_generate list and last_modified_domain"""
    # the logic here could be better... need to keep a list of succussful
    # batches so failed batches can be re-tried
    # but then also need a way to detect and mark pathological cases
    files_to_generate.append(url)
    print(url)
    attrs = {'last_modified': last_modified_on_oac_header , 'batch_id': BATCH }
    last_modified_domain.put_attributes(url, attrs)


def shadow(bucketurl, archive, prefix):
    """create shadow artifact for XTF index (so XTF can know what files are in the bucket and the PDF sizes"""
    parts = urlparse.urlsplit(bucketurl)  # SplitResult(scheme='s3', netloc='test.pdf', path='/dkd', query='', fragment='')
    s3 = boto.connect_s3()
    bucket = s3.get_bucket(parts.netloc)
    tmp = tempfile.NamedTemporaryFile(delete=False)
    tar = tarfile.open(fileobj=tmp, mode="w:gz")
    for key in bucket.list():
        # look for pdfs that match the user supplied path
        if key.name.endswith(u'.pdf') and not parts.path or key.name.startswith(parts.path[1:]):
            # write directly to a tar file http://stackoverflow.com/a/740839/1763984
            shadowfile = StringIO.StringIO()
            shadowfile.write(str(key.size))
            shadowfile.seek(0)
            shadowname = os.path.join(prefix, os.path.splitext(key.name)[0])
            info = tarfile.TarInfo(shadowname)
            info.size = len(shadowfile.buf)
            # boto last_modified to Datetime http://stackoverflow.com/a/9688496/1763984
            # Datetime to unixtime http://stackoverflow.com/a/255053/1763984
            info.mtime = time.mktime(boto.utils.parse_ts(key.last_modified).timetuple())
            tar.addfile(tarinfo=info, fileobj=shadowfile)
            shadowfile.close()
    tmp.flush()
    shutil.move(tmp.name, archive)
    os.chmod(archive, 0664)


def launch_ec2():
    connection = boto.connect_ec2()
    print "connected, about to reserve"
    reservation = connection.request_spot_instances(
        "1.00",         # bid at on-demand rate
        "ami-05355a6c", # check http://aws.amazon.com/amazon-linux-ami/ for current AMI
        key_name="majorTom-worker",
        #placement="us-east-1b",        #   should placement be more open ended?
        instance_profile_arn="arn:aws:iam::563907706919:instance-profile/s3-read-write",
        instance_type="m3.2xlarge",     # 1.00/hr on demand    8vCPU       26 ECPU     30 G RAM
    )
    spot_id = str(reservation[0].id)
    print spot_id
    Resholder = namedtuple('Resholder', 'instance_id status')
    spot_reservation = Resholder(None, 'jumpstarting')
    # poll for spot instance to start up
    while spot_reservation.instance_id == None:
        pp(spot_reservation.status)
        sleep(20)
        spot_reservation = connection.get_all_spot_instance_requests(spot_id)[0]
    instance = connection.get_all_instances(spot_reservation.instance_id)[0].instances[0]
    pp(instance)
    instance.add_tag("Name","OAC_pdfu")
    instance.add_tag("project","OAC_pdfu")
    if not instance.public_dns_name == None:
        print "needs hostname"
    while instance.public_dns_name == None:
        sleep(20)
        sys.stdout.write('·')
    hostname = instance.public_dns_name
    return spot_reservation.instance_id, hostname


def poll_for_ssh(host):
    # http://stackoverflow.com/a/2561727/1763984
    # Set the timeout
    original_timeout = socket.getdefaulttimeout()
    new_timeout = 3
    socket.setdefaulttimeout(new_timeout)
    host_status = False
    while not host_status:
        try:
            paramiko.Transport((host, 22))
            host_status = True
        except Exception as e:
            pp(e)
        sleep(20)
        sys.stdout.write('⋅')
    socket.setdefaulttimeout(original_timeout)
    return host_status


def terminate_ec2(instance):
    connection = boto.connect_ec2()
    return connection.get_all_instances(instance)[0].instances[0].terminate()


def remote_process_pdf(hostname, batch, instance):
    """use fabric to run commands on the remote working node"""
    SETUP_SUDO = [ 
        'yum -y update',
        'yum -y install git',
        'yum -y groupinstall "Development Tools"',
        'easy_install pip',
        'pip install --no-use-wheel virtualenv',
        'yum -y install python-devel',
        'yum -y install ncurses-devel',
        'yum -y install openssl-devel',
        'yum -y install libjpeg-devel',
        'yum -y install freetype-devel',
        'yum -y install libtiff-devel',
        'yum -y install lcms-devel',
        'yum -y install mercurial',
        'echo halt | at now + 36 hours',
    ]
    SETUP_RUN = [
        'curl -L https://raw.github.com/ucldc/appstrap/master/cdl/ucldc-operator-keys.txt >> ~/.ssh/authorized_keys',
        'git clone https://github.com/tingletech/pdfu.git',
        './pdfu/init.sh',
    ]
    env.host_string = hostname
    # fabric docs say fabric could hang if a command fails and recommend to use try/finally
    try:
        pp(SETUP_SUDO)
        for command in SETUP_SUDO:
            sudo(command)
        pp(SETUP_RUN)
        for command in SETUP_RUN:
            run(command)
        # TODO: need to deal with the fact that if the spot price exceeds the bid price
        # that the instance might die during the main run.  This might be a place things
        # stall out 
        put(batch, '/home/ec2-user/batch.txt')
        pp("remote xargs")
        run('xargs -a /home/ec2-user/batch.txt -P 7 -n 2 ./pdfu/pdfu')
        #    xargs                             -P 7          use n-1 processors (leaves one open for forked saxon)
        #    xargs                                  -n 2     pass 2 arguments at a time to command
    finally:
        fabric.network.disconnect_all()


def generate_batch(files_to_generate, eads, bucket):
    """turn the array of URLs into the args that will be passed to xargs on the remote node"""
    batch = StringIO.StringIO()
    for url in files_to_generate:
        args = "%s %s\n" % ( url, fixup_url(url, eads, bucket) )
        batch.write(args)
    batch.seek(0)
    return batch


def fixup_url(url, eads, bucket):
    """dirty path hacking specific to Online Archive of California here"""
    dir, ext = os.path.splitext(url)
    fixup = url.replace(eads,bucket)
    return u"%s.pdf" % ( os.path.splitext(fixup)[0] )

# main() idiom for importing into REPL for debugging 
if __name__ == "__main__":
    sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0) # http://stackoverflow.com/a/9462099/1763984
    sys.exit(main())


"""
   Copyright (c) 2014, Regents of the University of California
   All rights reserved.

   Redistribution and use in source and binary forms, with or without
   modification, are permitted provided that the following conditions are
   met:

   - Redistributions of source code must retain the above copyright notice,
     this list of conditions and the following disclaimer.
   - Redistributions in binary form must reproduce the above copyright
     notice, this list of conditions and the following disclaimer in the
     documentation and/or other materials provided with the distribution.
   - Neither the name of the University of California nor the names of its
     contributors may be used to endorse or promote products derived from
     this software without specific prior written permission.

   THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
   AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
   IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
   ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE
   LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
   CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
   SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
   INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
   CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
   ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
   POSSIBILITY OF SUCH DAMAGE.
"""
