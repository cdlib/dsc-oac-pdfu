#!/bin/env python
import argparse
from lxml.html import parse
import requests
import os, sys
import boto.utils
import boto
import datetime
from time import sleep
from fabric.api import env, run, sudo, put
import paramiko
import fabric
import StringIO
import socket
import urlparse
import tarfile
import time
import tempfile
import shutil

def main(argv=None):
    """ run a batch of PDF generation

    keeps an amazon simple DB with an inventory of EAD files and last
    modified dates, this persists between runs

    check_url is a recursive modifier function (with side effects) that is
    a web crawler that adds all the new EAD to an array `files_to_generate`
    and updates the amazon simple db as needed

    If there are files_to_generate, a spot purchase of an 8 core 30G RAM 
    is initiated, and fabric is used to install `pdfu` and run the batch
    in parallel using the -P option on xargs.

    Once the batch has run, the spot machine is terminated.

    The shadow file is regenerated even if there were no files to generate detected.

    July 1, 2013 is hardcoded as the epic for file changes, because a
    complete batch was run at this time.

    """

    parser = argparse.ArgumentParser(description="run the PDF batch")
    parser.add_argument('eads', nargs=1, help="URL for crawler to start harvesting EAD XML, won't follow redirects")
    parser.add_argument('bucket', nargs=1, help="s3://bucket[/optional/path] where the generated PDF files go")
    parser.add_argument('shadow', nargs=1, help=".tar.gz filename to store \"shadow\" file archive for XTF")
    parser.add_argument('--simpledb-domain', default='ead_last_modified', required=False, help="\"domain\"/name of Amazon Simple DB database")
    parser.add_argument('--shadow-prefix', default='pdf-shadow', required=False, help="path the .tar.gz will unpack to")

    if argv is None:
        argv = parser.parse_args()

    sdb = boto.connect_sdb()  	# amazon simple db
    last_modified_domain = sdb.get_domain(argv.simpledb_domain)
    files_to_generate = []
    check_url(argv.eads[0], last_modified_domain, files_to_generate)
    if files_to_generate:
        batch = generate_batch(files_to_generate, argv.eads[0], argv.bucket[0])
        instance, hostname = launch_ec2()
        poll_for_ssh(hostname)
        remote_process_pdf(hostname, batch)
        terminate_ec2(instance)
    shadow(argv.bucket[0], argv.shadow[0], argv.shadow_prefix)
    
def check_url(url, last_modified_domain, files_to_generate):
    """check if a URL is an XML file or directory based on the string value """
    dir, ext = os.path.splitext(url)
    # prime 2002 directory will have only .xml files or sub-directories
    if ext == '.xml':
        check_xml(url, last_modified_domain, files_to_generate)
    elif not ext:
        check_dir(url, last_modified_domain, files_to_generate)

def check_dir(url, last_modified_domain, files_to_generate):
    """scrape links from directory listing"""
    doc = parse(url).getroot()
    doc.make_links_absolute()
    links = doc.xpath("//a[@href]/@href")
    for link in links:
        # skip links back to myself and don't go up directories
        if not link == url and link.startswith(url):
            check_url(link, last_modified_domain, files_to_generate)

def check_xml(url, last_modified_domain, files_to_generate):
    """compare last_modifed in head with value in  simple DB to see if this needs processing"""
    r = requests.head(url)
    last_modified_on_oac_header = r.headers['last-modified']
    last_modified_on_oac = boto.utils.parse_ts(last_modified_on_oac_header)
    # if the URL is not in the last_modified_domain OR last-modified is later than the database; add
    # the file to the list of files to be processed
    last_modified_item = last_modified_domain.get_item(url)
    if not last_modified_item:
        if last_modified_on_oac > datetime.datetime(2013,07,01):
            add_to_list(url, last_modified_domain, files_to_generate, last_modified_on_oac_header)
    elif last_modified_on_oac > boto.utils.parse_ts(last_modified_item["last_modified"]):
        add_to_list(url, last_modified_domain, files_to_generate)
        
def add_to_list(url, last_modified_domain, files_to_generate, last_modified_on_oac_header):
    """modify the files_to_generate list and last_modified_domain"""
    files_to_generate.append(url)
    attrs = {'last_modified': last_modified_on_oac_header , 'batch_id': 'testing' }
    last_modified_domain.put_attributes(url, attrs)

def shadow(bucketurl, archive, prefix):
    """create shadow artifact for XTF index (so XTF can know what files are in the bucket and the PDF sizes"""
    parts = urlparse.urlsplit(bucketurl)  # SplitResult(scheme='s3', netloc='test.pdf', path='/dkd', query='', fragment='')
    s3 = boto.connect_s3()
    bucket = s3.get_bucket(parts.netloc)
    tmp = tempfile.NamedTemporaryFile(delete=False)
    tar = tarfile.open(fileobj=tmp, mode="w:gz")
    for key in bucket.list():
        # look for pdfs that match the user supplied path
        if key.name.endswith(u'.pdf') and not parts.path or key.name.startswith(parts.path[1:]):
            # write directly to a tar file http://stackoverflow.com/a/740839/1763984
            shadowfile = StringIO.StringIO()
            shadowfile.write(str(key.size))
            shadowfile.seek(0)
            shadowname = os.path.join(prefix, os.path.splitext(key.name)[0])
            info = tarfile.TarInfo(shadowname)
            info.size = len(shadowfile.buf)
            # boto last_modified to Datetime http://stackoverflow.com/a/9688496/1763984
            # Datetime to unixtime http://stackoverflow.com/a/255053/1763984
            info.mtime = time.mktime(boto.utils.parse_ts(key.last_modified).timetuple())
            tar.addfile(tarinfo=info, fileobj=shadowfile)
            shadowfile.close()
    tmp.flush()
    shutil.move(tmp.name, archive)

def launch_ec2():
    connection = boto.connect_ec2()
    reservation = connection.request_spot_instances(
                                       "1.00",         # bid at on-demand rate
                                       "ami-05355a6c", # check http://aws.amazon.com/amazon-linux-ami/ for current AMI
                                       key_name="majorTom-keypair",
                                       placement="us-east-1b",        #   should placement be more open ended?
                                       instance_profile_arn="arn:aws:iam::563907706919:instance-profile/s3-read-write",
                                       instance_type="m3.2xlarge")   #   1.00/hr on demand    8vCPU       26 ECPU     30 G RAM
    spot_id = str(reservation[0].id)
    spot_reservation = connection.get_all_spot_instance_requests(spot_id)[0]
    # poll for spot instance to start up
    while spot_reservation.instance_id == None:
        sleep(20)
        spot_reservation = connection.get_all_spot_instance_requests(spot_id)[0]
    instance = connection.get_all_instances(spot_reservation.instance_id)[0].instances[0]
    instance.add_tag("Name","OAC_pdfu")
    hostname = instance.public_dns_name
    return spot_reservation.instance_id, hostname

def poll_for_ssh(host):
    # http://stackoverflow.com/a/2561727/1763984
    # Set the timeout
    original_timeout = socket.getdefaulttimeout()
    new_timeout = 3
    socket.setdefaulttimeout(new_timeout)
    host_status = False
    while not host_status:
        try:
            paramiko.Transport((host, 22))
            host_status = True
        except:
            pass
        sleep(20)
    socket.setdefaulttimeout(original_timeout)
    return host_status

def terminate_ec2(instance):
    connection = boto.connect_ec2()
    return connection.get_all_instances(instance)[0].instances[0].terminate()

def remote_process_pdf(hostname, batch):
    """use fabric to run commands on the remote working node"""
    SETUP_SUDO = [ 
        'yum -y update',
        'yum -y install git',
        'yum -y groupinstall "Development Tools"',
        'easy_install pip',
        'pip install virtualenv',
        'yum -y install python-devel',
        'yum -y install ncurses-devel',
        'yum -y install openssl-devel',
        'yum -y install libjpeg-devel',
        'yum -y install freetype-devel',
        'yum -y install libtiff-devel',
        'yum -y install lcms-devel',
        'yum -y install mercurial',
    ]
    SETUP_RUN = [
        'curl -L https://raw.github.com/ucldc/appstrap/master/cdl/ucldc-operator-keys.txt >> ~/.ssh/authorized_keys',
        'git clone https://github.com/tingletech/pdfu.git',
        './pdfu/init.sh',
    ]
    env.host_string = hostname
    # fabric docs say fabric could hang if a command fails and recommend to use try/finally
    try:
        for command in SETUP_SUDO:
            sudo(command)
        for command in SETUP_RUN:
            run(command)
        put(batch, '/home/ec2-user/batch.txt')
        run('xargs -a /home/ec2-user/batch.txt -P 7 -n 2 ./pdfu/pdfu')
        #    xargs                             -P 7          use n-1 processors (leaves one open for forked saxon)
        #    xargs                                  -n 2     pass 2 arguments at a time to command
    finally:
        fabric.network.disconnect_all()

def generate_batch(files_to_generate, eads, bucket):
    """turn the array of URLs into the args that will be passed to xargs on the remote node"""
    batch = StringIO.StringIO()
    for url in files_to_generate:
        args = "%s %s\n" % ( url, fixup_url(url, eads, bucket) )
        batch.write(args)
    batch.seek(0)
    return batch

def fixup_url(url, eads, bucket):
    """dirty path hacking specific to Online Archive of California here"""
    dir, ext = os.path.splitext(url)
    fixup = url.replace(eads,bucket)
    return u"%s.pdf" % ( os.path.splitext(fixup)[0] )

# main() idiom for importing into REPL for debugging 
if __name__ == "__main__":
    sys.exit(main())
